# Testing Without API Keys

This guide explains how to test and validate your bioinformatics semantic search system without requiring any external API keys.

## Prerequisites

1. Make sure you have installed all the required dependencies:
   ```
   pip install -r requirements.txt
   ```

2. Start the Qdrant vector database:
   ```
   docker run -d -p 6333:6333 -p 6334:6334 qdrant/qdrant
   ```

3. Create the vector collection:
   ```
   python qdrant_db/create_collection.py
   ```

4. Upload the tool data:
   ```
   python qdrant_db/upload_data.py
   ```

## Testing Options

### Option 1: Test Vector Search Only

This test validates that your vector search is working correctly. It doesn't require any LLM or API keys.

```
python test_vector_search.py
```

This script will:
1. Load the biomedical embeddings model
2. Connect to your Qdrant database
3. Run several test queries
4. Display the most semantically similar tools for each query

### Option 2: Test with a Small Local LLM

This test validates the entire RAG (Retrieval Augmented Generation) pipeline using a small, free LLM that runs locally instead of requiring API keys.

```
python test_with_local_llm.py
```

This script will:
1. Load the biomedical embeddings model
2. Connect to your Qdrant database
3. Download and run a small local language model (TinyLlama-1.1B)
4. Set up the complete RAG pipeline
5. Run a test query through the entire system
6. Show you the response generated by the local LLM

**Note**: The small local model won't give responses as high quality as Gemini or other commercial models, but it demonstrates that your pipeline works end-to-end.

## Hardware Requirements

- **Vector Search Only**: Any modern computer should be able to run the vector search test
- **Local LLM Test**: Requires at least 4GB of RAM, and runs much better with a GPU

## Troubleshooting

### Common Issues

1. **Qdrant connection error**:
   - Make sure the Qdrant container is running: `docker ps`
   - Check Qdrant logs: `docker logs [container_id]`

2. **Out of memory when loading the local LLM**:
   - Use a smaller model by editing `test_with_local_llm.py`
   - Close other applications to free memory
   - If available, enable swap space

3. **Slow response times**:
   - This is normal for the local LLM test on CPU
   - The full system with Gemini or OpenAI will be much faster

## Next Steps

Once you've confirmed everything works:

1. Get API keys for Gemini or OpenAI
2. Add them to your `.env` file
3. Run the full bioinformatics assistant:
   ```
   # For Gemini
   python bioinfo_assistant_gemini.py
   
   # For OpenAI (if you implemented that version)
   python bioinfo_assistant.py
   ``` 